<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0JKBJ3WRJZ"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-0JKBJ3WRJZ');
    </script>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3&display=swap" rel="stylesheet">
    <meta charset="UTF-8">
    <title>DeepAudio-V1</title>

    <link rel="icon" type="image/png" href="images/icon.png">

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- CSS only -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-+0n0xVW2eSR5OomGNYDnhzAbDsOXxcvSN1TPprVMTNDbiYZCxYbOOl7+AMvyTG2x" crossorigin="anonymous">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

    <link rel="stylesheet" href="style.css">
</head>
<body>

    <body>
        <br><br><br><br>
        <div class="container">
            <div class="row text-center" style="font-size:38px">
                <div class="col strong">
                    DeepAudio-V1:Towards Multi-Modal Multi-Stage End-to-End <br>Video to Speech and Audio Generation
                </div>
            </div>
			
            <br>
            <br>
    
            <div class="h-100 row text-center justify-content-md-center" style="font-size:20px;">
                <div class="col-sm-2">
                    <a href="https://arxiv.org">[Paper]</a>
                </div>
                <div class="col-sm-2">
                    <a href="https://github.com/acappemin/DeepAudio-V1">[Code]</a>
                </div>
                <div class="col-sm-3">
                    <a href="https://huggingface.co/spaces/lshzhm/DeepAudio-V1">[Huggingface Demo]</a>
                </div>
            </div>
			
			<br>
            <br>
			
			<hr>
			
			<br>
            <br>
			
			<div class="row" style="font-size:32px">
                <div class="col strong">
                    Abstract
                </div>
            </div>
			
			<br>
            <br>
			
			Currently, high-quality, synchronized audio is synthesized using various multi-modal joint learning frameworks, leveraging video and optional text inputs. In the video-to-audio benchmarks, video-to-audio quality, semantic alignment, and audio-visual synchronization are effectively achieved.However, in real-world scenarios, speech and audio often coexist in videos simultaneously, and the end-to-end generation of synchronous speech and audio given video and text conditions are not well studied. Therefore, we propose an end-to-end multi-modal generation framework that simultaneously produces speech and audio based on video and text conditions. Furthermore, the advantages of video-to-audio (V2A) models for generating speech from videos remain unclear. The proposed framework, DeepAudio, consists of a video-to-audio (V2A) module, a text-to-speech (TTS) module, and a dynamic mixture of modality fusion (MoF) module. In the evaluation, the proposed end-to-end framework achieves state-of-the-art performance on the video-audio benchmark, video-speech benchmark, and text-speech benchmark. In detail, our framework achieves comparable results in the comparison with state-of-the-art models for the video-audio and text-speech benchmarks, and surpassing state-of-the-art models in the video-speech benchmark, with WER 16.57% to 3.15% (+80.99%), SPK-SIM 78.30% to 89.38% (+14.15%), EMO-SIM 66.24% to 75.56% (+14.07%), MCD 8.59 to 7.98 (+7.10%), MCD_SL 11.05 to 9.40 (+14.93%) across a variety of dubbing settings.
			
            <br>
            <br>
			
            <hr>
			
			<br>
            <br>
			
			<div class="row" style="font-size:32px">
                <div class="col strong">
                    Method
                </div>
            </div>
			
			<br>
            <br>
            
			<img src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/images/Fig_2_2.png" alt="overview" style="width:100%;">
			
			<br>
			
			Figure 1: The overview of DeepAudio framework. The proposed DeepAudio framework unifies video-to-audio (V2A) and video-to-speech (V2S) generation in a multi-stage, end-to-end paradigm. The top section illustrates two independent generation paths: (1) The V2A module, which synthesizes ambient audio from video input using a CLIP-based multi-modal feature encoding and a noised latent representation, and (2) The TTS module, which generates speech conditioned on text and noised latent features. Both modules rely on codec decoders to reconstruct high-fidelity outputs. The bottom section presents the MoF module, an integrated multi-modal system that takes text, video, and instructions as inputs. A Gating Network adaptively fuses outputs from the V2A module and the TTS module, ensuring synchronized and context-aware audio-visual generation.
			
			<br>
            <br>
			
			<img src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/images/specs_3.jpg" alt="spectrograms" style="width:100%;">
			
			<br>
			
			Figure 2: Mel-spectrograms of ground truth and synthesized audio samples from different methods under V2C-Animation Dub 2.0 setting.
			
			<br>
            <br>
			
			<hr>
			
			<br>
            <br>
			
            <div class="row" style="font-size:32px">
                <div class="col strong">
                    Demos
                </div>
            </div>
			
            <br>
			<br>
			
			<div style="padding: 0 0; text-align: center; display: flex; justify-content: space-around;">
				<p style="text-align: center;">Ground Truth</p>
				<p style="text-align: center;">Generated Audios</p>
			</div>
			<div class="row">
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000001.gt.mp4?v=1.0">
				</video>
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000001.gen.mp4?v=1.0">
				</video>
			</div>
			<div class="row">
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000003.gt.mp4">
				</video>
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000003.gen.mp4">
				</video>
			</div>
			<div class="row">
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000024.gt.mp4">
				</video>
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000024.gen.mp4">
				</video>
			</div>
			<div class="row">
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000025.gt.mp4">
				</video>
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000025.gen.mp4">
				</video>
			</div>
			<div class="row">
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000111.gt.mp4">
				</video>
				<video style="width: 49%;" controls>
					<source src="https://acappemin.github.io/DeepAudio-V1.github.io/assets/videos/00000111.gen.mp4">
				</video>
			</div>


    
            <br><br>
            <br><br>
    
        </div>

</body>
</html>